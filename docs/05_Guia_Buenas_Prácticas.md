# ğŸ§­ GuÃ­a de Buenas PrÃ¡cticas â€“ WS_Data_Engineering

---

## 1. Objetivo
Establecer lineamientos tÃ©cnicos y metodolÃ³gicos para el desarrollo de notebooks, pipelines y procesos dentro de **Microsoft Fabric**, asegurando consistencia, mantenibilidad y desempeÃ±o.

---

## 2. Estructura general de carpetas

```text
ğŸ“¦ WS_Data_Engineering/
â”œâ”€ .github/
â”‚  â”œâ”€ COPILOT_INSTRUCTIONS.md
â”‚  â”œâ”€ ISSUE_TEMPLATE/
â”‚  â”‚   â”œâ”€ bug_report.md
â”‚  â”‚   â””â”€ feature_request.md
â”‚  â”œâ”€ PULL_REQUEST_TEMPLATE.md
â”‚  â””â”€ workflows/
â”‚      â””â”€ ci-pipeline.yml
â”‚
â”œâ”€ docs/
â”‚  â”œâ”€ 01_Alcance_Proyecto.md
â”‚  â”œâ”€ 02_Estandares_Naming.md
â”‚  â”œâ”€ 03_Estructura_Notebooks.md
â”‚  â”œâ”€ 04_Estructura_Pipelines.md
â”‚  â””â”€ 05_Guia_Commit_y_Versionado.md
â”‚
â”œâ”€ src/
â”‚  â”œâ”€ ingestion/
â”‚  â”œâ”€ transform/
â”‚  â”œâ”€ standardization/
â”‚  â””â”€ gold/
â”‚
â”œâ”€ tests/
â”‚  â””â”€ unit/
â”‚
â”œâ”€ README.md
â”œâ”€ requirements.txt
â””â”€ .gitignore

```


---

## 3. Control de versiones
- Utilizar ramas descriptivas:
  - `main`: rama estable.
  - `dev`: rama de desarrollo.
  - `feature/<nombre>`: nuevas funciones.
  - `hotfix/<nombre>`: correcciones crÃ­ticas.
- Versionar notebooks y pipelines con formato `vX.Y.Z`.
- Registrar cambios en `CHANGELOG.md`.

---

## 4. Logs y auditorÃ­a
- Usar el formato estÃ¡ndar de logs:
```text
[INFO] - 2025-10-27 10:00:00 - NB_Ingest_TiendasON - Ingesta completada exitosamente
[ERROR] - 2025-10-27 10:01:00 - NB_Ingest_TiendasON - Error al conectar con SQL Server
```
- Mantener tabla centralizada `Audit_Log` con los campos:
- `operation`, `table_name`, `primary_key`, `field`, `old_value`, `new_value`, `timestamp`, `user`
- Evitar `print()` en notebooks; usar `logging` o `log4j` cuando aplique.

---

## 5. Control de errores
- Usar bloques `try/except` con manejo de errores especÃ­ficos.
- Registrar errores crÃ­ticos con detalle de stacktrace.
- Incluir fallback (por ejemplo, â€œretry connectionâ€) cuando sea posible.
- Evitar detener el pipeline completo si una tabla falla: aislar procesos.

---

## 6. Performance y optimizaciÃ³n
- Repartir datos grandes usando `repartition()` o `coalesce()`.
- Evitar acciones costosas (`collect()`, `toPandas()`) en datasets grandes.
- Usar formatos **Delta** y **Parquet** segÃºn la etapa.
- En joins, preferir `broadcast()` cuando una tabla es pequeÃ±a.
- Aplicar `optimize` y `vacuum` periÃ³dicamente en Delta Lake.

---

## 7. Seguridad y configuraciÃ³n
- No incluir credenciales en notebooks ni pipelines.
- Usar **Microsoft Entra ID** o **Service Principals** para conexiones.
- Centralizar credenciales en **Fabric Linked Connections**.
- Proteger las rutas del Lakehouse segÃºn capas:
- Bronze: acceso restringido a desarrolladores.
- Silver y Gold: lectura controlada para analistas.

---

## 8. DocumentaciÃ³n
- Cada notebook y pipeline debe documentarse segÃºn los formatos de `/docs`.
- Mantener autor, versiÃ³n y fecha actualizada.
- Incluir comentarios inline en cÃ³digo con formato:
```python
# ğŸ”¹ Paso 1: Lectura incremental desde SQL Server
```

---

## 9. MÃ©tricas de ejecuciÃ³n
- Registrar en logs:
  - Tiempos de inicio y fin
  - Cantidad de registros procesados
  - DuraciÃ³n total
- Mantener una tabla histÃ³rica de ejecuciones (opcional):
`Execution_Log (process_name, start_time, end_time, status, records, duration)`

---

## 10. Recomendaciones de desarrollo
- Reutilizar funciones comunes (crear mÃ³dulo utils/).
- Mantener notebooks con menos de 500 lÃ­neas; dividir por procesos.
- Versionar notebooks exportÃ¡ndolos en .ipynb y .py para trazabilidad.
- Validar tipos de datos antes de escribir en Delta.

---

## 11. Escalabilidad futura
- Implementar orquestaciÃ³n dinÃ¡mica con FastAPI + APScheduler.
- Evaluar uso de Fabric Data Activator o Synapse Dataflows para automatizaciones.
- Adoptar patrones de ingestiÃ³n modular (por dominio o producto).
